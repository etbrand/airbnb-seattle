{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Introduction](#intro)   \n",
    "2. [Quantitative features](#quants)  \n",
    "3. [Ordinal features](#ordinal)  \n",
    "4. [Categorical features](#cats)  \n",
    "5. [Text features](#text)  \n",
    "6. [The full preprocessing and modeling pipeline](#pipeline)  \n",
    "7. [Model performance](#performance)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build a pipeline to preprocess features and train an XGBoost model to predict property price. We will combine sub-pipelines for different kinds of features with different preprocessing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/listings_train.csv', index_col='id', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the issues we saw in the price data in the `listings.csv` file, we'll use price data from the `calendar.csv` file as our target feature, instead. For now, we'll pick a fixed weekend date that isn't too far beyond when they calendars were scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_train = pd.read_csv('data/calendar_train_price.csv')\n",
    "y_train = calendar_train[calendar_train.date == '2019-08-03'].set_index('listing_id').loc[X_train.index].price.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantitative features\n",
    "<a id='quants'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert percent strings to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_to_float(pct_column):\n",
    "    \"\"\"Strip punctuation from percents and convert to floats\"\"\"\n",
    "    float_pct = [float(str(pct).replace('%', '')) for pct in pct_column]\n",
    "    return float_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.host_response_rate = pct_to_float(X_train.host_response_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `days_as_host` feature from `host_since` column, which is the date the host joined the site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['days_as_host'] = (pd.to_datetime('2019-07-14') - pd.to_datetime(X_train.host_since)).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_features = ['days_as_host', 'host_response_rate', 'host_listings_count', 'accommodates', 'bathrooms', 'bedrooms',\n",
    "                  'beds', 'guests_included', 'minimum_nights', 'number_of_reviews', 'review_scores_rating',\n",
    "                  'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                  'review_scores_communication', 'review_scores_location', 'review_scores_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer to select the quantitative data that can be used in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quant_features = FunctionTransformer(lambda x: x[quant_features], validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ordinal features\n",
    "<a id='ordinal'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode levels of ordinal features to preserve their order (treating binary features as ordinal features with only two levels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['host_is_superhost', 'instant_bookable', 'host_response_time', 'cancellation_policy', 'room_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_mapping = {'host_is_superhost': {'t': 1, 'f': 0},\n",
    "                   'instant_bookable': {'t': 1, 'f': 0},\n",
    "                   'host_response_time':\n",
    "                   {'within an hour': 4, 'within a few hours': 3, 'within a day': 2, 'a few days or more': 1},\n",
    "                   'cancellation_policy':\n",
    "                   {'super_strict_60': 1, 'super_strict_30': 2, 'strict': 3, 'strict_14_with_grace_period': 3, \\\n",
    "                    'moderate': 4, 'flexible': 5},\n",
    "                   'room_type': {'Entire home/apt': 3, 'Private room': 2, 'Shared room': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transformer to recode ordinal variables in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_ordinals(df):\n",
    "    df_recoded = df.replace(ordinal_mapping)\n",
    "    return df_recoded[ordinal_features]\n",
    "recode_ordinals = FunctionTransformer(recode_ordinals, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Categorical features\n",
    "<a id='cats'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simply use one-hot encoding for categorical features that have no obvious ordering. We can do this by converting these features to dictionaries and then using `DictVectorizer()` later in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['neighbourhood_group_cleansed', 'property_type']\n",
    "cat_to_dict = FunctionTransformer(lambda x: x[cat_features].to_dict('records'), validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Text features\n",
    "<a id='text'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some simple preprocessing of the text data, first combining all of the text columns from the dataframe together and then later using TfidfVectorizer() in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [word.lemma_ for word in doc if word.is_alpha and not word.is_stop]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    \"\"\"Combine text columns into a single text feature\"\"\"    \n",
    "    text_columns = ['name', 'summary', 'space', 'notes', 'amenities',\n",
    "                    'description', 'neighborhood_overview']\n",
    "    text_df = df[text_columns].replace(np.nan, '')\n",
    "    text_feature = text_df.apply(lambda x: ' '.join(x), axis=1).apply(preprocess_text)\n",
    "    return text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_features = FunctionTransformer(combine_text, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. The full preprocessing and modeling pipeline\n",
    "<a id='pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the full preprocessing and modeling pipeline, which uses FeatureUnion() to combine sub-pipelines for the different data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('quant_features', Pipeline([\n",
    "                    ('selector', get_quant_features),\n",
    "                    ('quant_imp', SimpleImputer())\n",
    "                ])),\n",
    "                ('ordinal_features', Pipeline([\n",
    "                    ('recode_ordinals', recode_ordinals),\n",
    "                    ('ordinal_imp', SimpleImputer(strategy='most_frequent'))\n",
    "                ])),\n",
    "                ('cat_features', Pipeline([\n",
    "                    ('to_dict', cat_to_dict),\n",
    "                    ('dict_vectorizer', DictVectorizer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('combine_text', get_text_features),\n",
    "                    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                    ('dim_red', TruncatedSVD(100))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('reg', XGBRegressor(n_estimators=50))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now preprocess the training data and fit an XGBoost model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('union',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('quant_features',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('selector',\n",
       "                                                                  FunctionTransformer(accept_sparse=False,\n",
       "                                                                                      check_inverse=True,\n",
       "                                                                                      func=<function <lambda> at 0x000001FC1A86C2F0>,\n",
       "                                                                                      inv_kw_args=None,\n",
       "                                                                                      inverse_func=None,\n",
       "                                                                                      kw_args=None,\n",
       "                                                                                      pass_y='deprecated',\n",
       "                                                                                      validate=False)),\n",
       "                                                                 ('quant_imp',\n",
       "                                                                  SimpleImput...\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "                              importance_type='gain', learning_rate=0.1,\n",
       "                              max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                              missing=None, n_estimators=50, n_jobs=1,\n",
       "                              nthread=None, objective='reg:linear',\n",
       "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                              scale_pos_weight=1, seed=None, silent=True,\n",
       "                              subsample=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model performance\n",
    "<a id='performance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate our model's performance by seeing how well it does on the test data. Since our pipeline takes care of most of the preprocessing, we will have to do very little to the raw test data before we feed it into the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('data/listings_test.csv', index_col='id', low_memory=False)\n",
    "X_test.host_response_rate = pct_to_float(X_test.host_response_rate)\n",
    "X_test['days_as_host'] = (pd.to_datetime('2019-07-14') - pd.to_datetime(X_test.host_since)).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_test = pd.read_csv('data/calendar_test.csv')\n",
    "y_test = calendar_test[calendar_test.date == '2019-08-03'].set_index('listing_id').loc[X_test.index].price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the model: 81.0\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred) ** .5\n",
    "print('RMSE of the model: {0:.1f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we can compare this to a baseline model where we always just predict the mean price for all properties we had in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 142.6\n"
     ]
    }
   ],
   "source": [
    "rmse_baseline = np.mean((np.mean(y_train) - y_test) ** 2) ** .5\n",
    "print('Baseline RMSE: {0:.1f}'.format(rmse_baseline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
